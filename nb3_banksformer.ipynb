{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swymtxpl7W7w"
   },
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook can train a model to generate sythetic data.   \n",
    "Ensure the 'ds_suffix' matches the one used to generated the dataset (Under \"Set input dataset\" & in create_dataset notebook)  \n",
    "Parameters for generating data (seq_len, number of seqs) are near bottom (Under \"Generate Full dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JjJJyJTZYebt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pXzVhU34zWEU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)  # suppress warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cCvXbPkccV1"
   },
   "source": [
    "### Set input dataset and nb_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLOCK_DIMS = data_encoder.field_info[\"CLOCK_DIMS\"]\n",
      "DATA_KEY_ORDER = data_encoder.field_info[\"DATA_KEY_ORDER\"]\n",
      "LOSS_TYPES = data_encoder.field_info[\"LOSS_TYPES\"]\n"
     ]
    }
   ],
   "source": [
    "from my_lib.encoding import load_data_encoder\n",
    "\n",
    "data_encoder = load_data_encoder()\n",
    "\n",
    "vars_to_load =  ['CLOCK_DIMS', 'DATA_KEY_ORDER', 'LOSS_TYPES']\n",
    "\n",
    "\n",
    "for var in vars_to_load:\n",
    "    cmd = f'{var} = data_encoder.field_info[\"{var}\"]'\n",
    "    print(cmd)\n",
    "    exec(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp_tensor_tr = np.load(f\"stored_data/inp_tensor_tr.npy\")\n",
      "inp_tensor_cv = np.load(f\"stored_data/inp_tensor_cv.npy\")\n",
      "tar_tensor_tr = np.load(f\"stored_data/tar_tensor_tr.npy\")\n",
      "tar_tensor_cv = np.load(f\"stored_data/tar_tensor_cv.npy\")\n",
      "inds_tr = np.load(f\"stored_data/inds_tr.npy\")\n",
      "inds_cv = np.load(f\"stored_data/inds_cv.npy\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((11483, 81, 27), (11483, 80, 9), (14354,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for data in ['inp_tensor', 'tar_tensor', 'inds']:\n",
    "    for suffix in [\"tr\", \"cv\"]:\n",
    "        file = f'{data}_{suffix}'\n",
    "        cmd = f'{file} = np.load(f\"stored_data/{file}.npy\")'\n",
    "        print(cmd)\n",
    "        exec(cmd)\n",
    "\n",
    "\n",
    "attributes = np.load(f\"stored_data/attributes.npy\")\n",
    "\n",
    "inp_tensor_tr.shape, tar_tensor_tr.shape, attributes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2871, 81, 27), (2871, 80, 9))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_tensor_cv.shape, tar_tensor_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, n_steps, n_feat_inp = inp_tensor_tr.shape\n",
    "n_feat_tar = tar_tensor_tr.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tf dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-13 20:08:17.066770: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset element_spec=(TensorSpec(shape=(81, 27), dtype=tf.float32, name=None), TensorSpec(shape=(80, 9), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_tr = tf.data.Dataset.from_tensor_slices((inp_tensor_tr.astype(np.float32), tar_tensor_tr.astype(np.float32)))\n",
    "ds_cv = tf.data.Dataset.from_tensor_slices((inp_tensor_cv.astype(np.float32), tar_tensor_cv.astype(np.float32)))\n",
    "\n",
    "ds_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "BUN_jLBTwNxk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from my_lib.transformer_core import make_batches\n",
    "\n",
    "BUFFER_SIZE = ds_tr.cardinality().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7e7hKcxn6-zd"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9SoX0-vd1hue",
    "tags": []
   },
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# float32, used for training \n",
    "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
    "    log2pi = tf.math.log(2. * np.pi)\n",
    "    return  -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# float64, used for generation \n",
    "def log_normal_pdf_gen(sample, mean, logvar, raxis=1):\n",
    "    log2pi = tf.cast(tf.math.log(2. * np.pi), tf.float64)\n",
    "    return  -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ZOJUSB1T8GjM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy, MeanSquaredError, SparseCategoricalCrossentropy\n",
    "\n",
    "\n",
    "loss_scce_logit = SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "loss_scce_probit = SparseCategoricalCrossentropy(\n",
    "    from_logits=False, reduction='none')\n",
    "\n",
    "loss_mse = MeanSquaredError(reduction='none')\n",
    "\n",
    "\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(tf.reduce_sum(seq, axis=2), 0), tf.float32)\n",
    "\n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    loss_parts = []\n",
    "    loss_parts_weighted = []\n",
    "\n",
    "    for k, k_pred in pred.items():\n",
    "        \n",
    "#         print(k, k_pred.shape)\n",
    "\n",
    "        st = FIELD_STARTS_TAR[k]\n",
    "        end = st + FIELD_DIMS_TAR[k]\n",
    "        loss_type = LOSS_TYPES[k]\n",
    "        \n",
    "\n",
    "        if loss_type == \"scce\":\n",
    "            loss_ = loss_scce_logit(real[:, :, st:end], k_pred)\n",
    "        elif loss_type == \"clock\":\n",
    "            loss_ = loss_scce_probit(real[:, :, st:end], clock_to_onehot(k, k_pred))\n",
    "        elif loss_type == \"mse\":\n",
    "            loss_ = loss_mse(real[:, :, st:end], k_pred)\n",
    "        elif loss_type == \"pdf\":\n",
    "            loss_ = -log_normal_pdf(real[:, :, st:end], k_pred[:,:,0:1], k_pred[:,:,1:2])[:,:,0]\n",
    "        else:\n",
    "            raise Exception(f\"Invalid loss type! Got loss type = {loss_type} with key = {k}. Check field_config.py for loss types\")\n",
    "            \n",
    "\n",
    "        mask = tf.math.logical_not(tf.math.equal(tf.reduce_sum(real, axis=2), 0))\n",
    "        mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "        loss_ *= mask\n",
    "        loss_ = tf.reduce_sum(loss_)/tf.reduce_sum(mask) \n",
    "\n",
    "        loss_parts.append(loss_)\n",
    "        loss_parts_weighted.append(loss_ * LOSS_WEIGHTS[k])\n",
    "\n",
    "    return tf.reduce_sum(loss_parts_weighted), loss_parts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Banksformer configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# others are None\n",
    "ACTIVATIONS = {\n",
    "    \"td_sc\": \"relu\",\n",
    "    \"log_amount_sc\": \"relu\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "lnJn5SLA2ahP",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIELD_DIMS_IN = data_encoder.field_info[\"FIELD_DIMS_IN\"]\n",
      "FIELD_STARTS_IN = data_encoder.field_info[\"FIELD_STARTS_IN\"]\n",
      "FIELD_DIMS_TAR = data_encoder.field_info[\"FIELD_DIMS_TAR\"]\n",
      "FIELD_STARTS_TAR = data_encoder.field_info[\"FIELD_STARTS_TAR\"]\n",
      "FIELD_DIMS_NET = data_encoder.field_info[\"FIELD_DIMS_NET\"]\n",
      "FIELD_STARTS_NET = data_encoder.field_info[\"FIELD_STARTS_NET\"]\n"
     ]
    }
   ],
   "source": [
    "# vars_to_load =  ['DATA_KEY_ORDER', 'CLOCK_DIMS', 'INP_ENCODINGS', 'TAR_ENCODINGS']\n",
    "vars_to_load = ['FIELD_DIMS_IN', 'FIELD_STARTS_IN', 'FIELD_DIMS_TAR', 'FIELD_STARTS_TAR', 'FIELD_DIMS_NET', 'FIELD_STARTS_NET']\n",
    "\n",
    "for var in vars_to_load:\n",
    "    cmd = f'{var} = data_encoder.field_info[\"{var}\"]'\n",
    "    print(cmd)\n",
    "    exec(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "# FIELD_DIMS_IN, FIELD_STARTS_IN, FIELD_DIMS_TAR, FIELD_STARTS_TAR, FIELD_DIMS_NET, FIELD_STARTS_NET = get_field_info(ds_suffix)\n",
    "\n",
    "config[\"ORDER\"] = DATA_KEY_ORDER\n",
    "config[\"FIELD_STARTS_IN\"] = FIELD_STARTS_IN\n",
    "config[\"FIELD_DIMS_IN\"] = FIELD_DIMS_IN\n",
    "config[\"FIELD_STARTS_NET\"] = FIELD_STARTS_NET\n",
    "config[\"FIELD_DIMS_NET\"] = FIELD_DIMS_NET\n",
    "\n",
    "\n",
    "config[\"ACTIVATIONS\"] = ACTIVATIONS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "bbvmaKNiznHZ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:09\n",
      "Begin running v2b__nld_4-dm_128-nh_2-i_0-dr_0.1-opt_adam-lwi_0-bs_64\n",
      "Epoch 1 Batch 0 Loss 14.9234\n",
      "Epoch 1 Batch 50 Loss 11.9195\n",
      "Epoch 1 Batch 100 Loss 11.2842\n",
      "Epoch 1 Batch 150 Loss 10.9633\n",
      "Epoch 1 Loss 10.4828\n",
      "** on validation data loss is 7.5292\n",
      "Not recording acc: 'Transformer' object has no attribute 'acc_function'\n",
      "Time taken for 1 epoch: 161.98 secs\n",
      "\n",
      "Saving checkpoint for epoch 1 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-1\n",
      "Epoch 2 Batch 0 Loss 7.6099\n",
      "Epoch 2 Batch 50 Loss 7.4858\n",
      "Epoch 2 Batch 100 Loss 7.2283\n",
      "Epoch 2 Batch 150 Loss 7.0586\n",
      "Epoch 2 Loss 6.9592\n",
      "** on validation data loss is 6.3389\n",
      "Time taken for 1 epoch: 147.52 secs\n",
      "\n",
      "Saving checkpoint for epoch 2 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-2\n",
      "Epoch 3 Batch 0 Loss 6.4385\n",
      "Epoch 3 Batch 50 Loss 6.3202\n",
      "Epoch 3 Batch 100 Loss 6.2352\n",
      "Epoch 3 Batch 150 Loss 6.1492\n",
      "Epoch 3 Loss 6.0966\n",
      "** on validation data loss is 5.8037\n",
      "Time taken for 1 epoch: 150.32 secs\n",
      "\n",
      "Saving checkpoint for epoch 3 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-3\n",
      "Epoch 4 Batch 0 Loss 5.8771\n",
      "Epoch 4 Batch 50 Loss 5.5965\n",
      "Epoch 4 Batch 100 Loss 5.4602\n",
      "Epoch 4 Batch 150 Loss 5.3163\n",
      "Epoch 4 Loss 5.2374\n",
      "** on validation data loss is 4.7135\n",
      "Time taken for 1 epoch: 151.08 secs\n",
      "\n",
      "Saving checkpoint for epoch 4 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-4\n",
      "Epoch 5 Batch 0 Loss 4.7691\n",
      "Epoch 5 Batch 50 Loss 4.7205\n",
      "Epoch 5 Batch 100 Loss 4.6668\n",
      "Epoch 5 Batch 150 Loss 4.6072\n",
      "Epoch 5 Loss 4.5784\n",
      "** on validation data loss is 4.2921\n",
      "Time taken for 1 epoch: 151.60 secs\n",
      "\n",
      "Saving checkpoint for epoch 5 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-5\n",
      "Epoch 6 Batch 0 Loss 4.3610\n",
      "Epoch 6 Batch 50 Loss 4.3535\n",
      "Epoch 6 Batch 100 Loss 4.3223\n",
      "Epoch 6 Batch 150 Loss 4.2839\n",
      "Epoch 6 Loss 4.2655\n",
      "** on validation data loss is 4.2689\n",
      "Time taken for 1 epoch: 153.92 secs\n",
      "\n",
      "Saving checkpoint for epoch 6 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-6\n",
      "Epoch 7 Batch 0 Loss 4.3733\n",
      "Epoch 7 Batch 50 Loss 4.1348\n",
      "Epoch 7 Batch 100 Loss 4.1107\n",
      "Epoch 7 Batch 150 Loss 4.0989\n",
      "Epoch 7 Loss 4.0877\n",
      "** on validation data loss is 3.9633\n",
      "Time taken for 1 epoch: 151.65 secs\n",
      "\n",
      "Saving checkpoint for epoch 7 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-7\n",
      "Epoch 8 Batch 0 Loss 4.1974\n",
      "Epoch 8 Batch 50 Loss 3.9982\n",
      "Epoch 8 Batch 100 Loss 3.9898\n",
      "Epoch 8 Batch 150 Loss 3.9748\n",
      "Epoch 8 Loss 3.9704\n",
      "** on validation data loss is 3.8772\n",
      "Time taken for 1 epoch: 152.83 secs\n",
      "\n",
      "Saving checkpoint for epoch 8 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-8\n",
      "Epoch 9 Batch 0 Loss 3.8727\n",
      "Epoch 9 Batch 50 Loss 3.9045\n",
      "Epoch 9 Batch 100 Loss 3.9041\n",
      "Epoch 9 Batch 150 Loss 3.8954\n",
      "Epoch 9 Loss 3.8931\n",
      "** on validation data loss is 3.8372\n",
      "Time taken for 1 epoch: 148.73 secs\n",
      "\n",
      "Saving checkpoint for epoch 9 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-9\n",
      "Epoch 10 Batch 0 Loss 3.8790\n",
      "Epoch 10 Batch 50 Loss 3.8454\n",
      "Epoch 10 Batch 100 Loss 3.8392\n",
      "Epoch 10 Batch 150 Loss 3.8385\n",
      "Epoch 10 Loss 3.8359\n",
      "** on validation data loss is 3.8154\n",
      "Time taken for 1 epoch: 165.20 secs\n",
      "\n",
      "Saving checkpoint for epoch 10 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-10\n",
      "Epoch 11 Batch 0 Loss 3.9917\n",
      "Epoch 11 Batch 50 Loss 3.8115\n",
      "Epoch 11 Batch 100 Loss 3.7997\n",
      "Epoch 11 Batch 150 Loss 3.7940\n",
      "Epoch 11 Loss 3.7932\n",
      "** on validation data loss is 3.7554\n",
      "Time taken for 1 epoch: 155.08 secs\n",
      "\n",
      "Saving checkpoint for epoch 11 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-11\n",
      "Epoch 12 Batch 0 Loss 3.8010\n",
      "Epoch 12 Batch 50 Loss 3.7602\n",
      "Epoch 12 Batch 100 Loss 3.7602\n",
      "Epoch 12 Batch 150 Loss 3.7588\n",
      "Epoch 12 Loss 3.7595\n",
      "** on validation data loss is 3.6720\n",
      "Time taken for 1 epoch: 149.28 secs\n",
      "\n",
      "Saving checkpoint for epoch 12 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-12\n",
      "Epoch 13 Batch 0 Loss 3.8331\n",
      "Epoch 13 Batch 50 Loss 3.7369\n",
      "Epoch 13 Batch 100 Loss 3.7311\n",
      "Epoch 13 Batch 150 Loss 3.7299\n",
      "Epoch 13 Loss 3.7295\n",
      "** on validation data loss is 3.6806\n",
      "Time taken for 1 epoch: 148.29 secs\n",
      "\n",
      "Saving checkpoint for epoch 13 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-13\n",
      "Epoch 14 Batch 0 Loss 3.6885\n",
      "Epoch 14 Batch 50 Loss 3.7069\n",
      "Epoch 14 Batch 100 Loss 3.7080\n",
      "Epoch 14 Batch 150 Loss 3.7066\n",
      "Epoch 14 Loss 3.7060\n",
      "** on validation data loss is 3.6665\n",
      "Time taken for 1 epoch: 150.78 secs\n",
      "\n",
      "Saving checkpoint for epoch 14 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-14\n",
      "Epoch 15 Batch 0 Loss 3.7155\n",
      "Epoch 15 Batch 50 Loss 3.6921\n",
      "Epoch 15 Batch 100 Loss 3.6899\n",
      "Epoch 15 Batch 150 Loss 3.6877\n",
      "Epoch 15 Loss 3.6883\n",
      "** on validation data loss is 3.6512\n",
      "Time taken for 1 epoch: 150.94 secs\n",
      "\n",
      "Saving checkpoint for epoch 15 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-15\n",
      "Epoch 16 Batch 0 Loss 3.7453\n",
      "Epoch 16 Batch 50 Loss 3.6700\n",
      "Epoch 16 Batch 100 Loss 3.6668\n",
      "Epoch 16 Batch 150 Loss 3.6726\n",
      "Epoch 16 Loss 3.6698\n",
      "** on validation data loss is 3.6401\n",
      "Time taken for 1 epoch: 149.29 secs\n",
      "\n",
      "Saving checkpoint for epoch 16 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-16\n",
      "Epoch 17 Batch 0 Loss 3.5772\n",
      "Epoch 17 Batch 50 Loss 3.6568\n",
      "Epoch 17 Batch 100 Loss 3.6593\n",
      "Epoch 17 Batch 150 Loss 3.6565\n",
      "Epoch 17 Loss 3.6565\n",
      "** on validation data loss is 3.6227\n",
      "Time taken for 1 epoch: 149.97 secs\n",
      "\n",
      "Saving checkpoint for epoch 17 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-17\n",
      "Epoch 18 Batch 0 Loss 3.6141\n",
      "Epoch 18 Batch 50 Loss 3.6527\n",
      "Epoch 18 Batch 100 Loss 3.6440\n",
      "Epoch 18 Batch 150 Loss 3.6442\n",
      "Epoch 18 Loss 3.6435\n",
      "** on validation data loss is 3.6166\n",
      "Time taken for 1 epoch: 149.77 secs\n",
      "\n",
      "Saving checkpoint for epoch 18 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-18\n",
      "Epoch 19 Batch 0 Loss 3.6254\n",
      "Epoch 19 Batch 50 Loss 3.6367\n",
      "Epoch 19 Batch 100 Loss 3.6348\n",
      "Epoch 19 Batch 150 Loss 3.6343\n",
      "Epoch 19 Loss 3.6320\n",
      "** on validation data loss is 3.6054\n",
      "Time taken for 1 epoch: 150.16 secs\n",
      "\n",
      "Saving checkpoint for epoch 19 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-19\n",
      "Epoch 20 Batch 0 Loss 3.6453\n",
      "Epoch 20 Batch 50 Loss 3.6240\n",
      "Epoch 20 Batch 100 Loss 3.6220\n",
      "Epoch 20 Batch 150 Loss 3.6258\n",
      "Epoch 20 Loss 3.6246\n",
      "** on validation data loss is 3.6089\n",
      "Time taken for 1 epoch: 150.54 secs\n",
      "\n",
      "Saving checkpoint for epoch 20 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-20\n",
      "Epoch 21 Batch 0 Loss 3.7025\n",
      "Epoch 21 Batch 50 Loss 3.6185\n",
      "Epoch 21 Batch 100 Loss 3.6141\n",
      "Epoch 21 Batch 150 Loss 3.6155\n",
      "Epoch 21 Loss 3.6154\n",
      "** on validation data loss is 3.5888\n",
      "Time taken for 1 epoch: 149.87 secs\n",
      "\n",
      "Saving checkpoint for epoch 21 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-21\n",
      "Epoch 22 Batch 0 Loss 3.6074\n",
      "Epoch 22 Batch 50 Loss 3.6118\n",
      "Epoch 22 Batch 100 Loss 3.6088\n",
      "Epoch 22 Batch 150 Loss 3.6072\n",
      "Epoch 22 Loss 3.6059\n",
      "** on validation data loss is 3.5830\n",
      "Time taken for 1 epoch: 149.58 secs\n",
      "\n",
      "Saving checkpoint for epoch 22 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-22\n",
      "Epoch 23 Batch 0 Loss 3.6042\n",
      "Epoch 23 Batch 50 Loss 3.5979\n",
      "Epoch 23 Batch 100 Loss 3.5968\n",
      "Epoch 23 Batch 150 Loss 3.6000\n",
      "Epoch 23 Loss 3.5992\n",
      "** on validation data loss is 3.5755\n",
      "Time taken for 1 epoch: 148.68 secs\n",
      "\n",
      "Saving checkpoint for epoch 23 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-23\n",
      "Epoch 24 Batch 0 Loss 3.5565\n",
      "Epoch 24 Batch 50 Loss 3.5904\n",
      "Epoch 24 Batch 100 Loss 3.5949\n",
      "Epoch 24 Batch 150 Loss 3.5909\n",
      "Epoch 24 Loss 3.5922\n",
      "** on validation data loss is 3.5710\n",
      "Time taken for 1 epoch: 151.01 secs\n",
      "\n",
      "Saving checkpoint for epoch 24 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-24\n",
      "Epoch 25 Batch 0 Loss 3.5450\n",
      "Epoch 25 Batch 50 Loss 3.5956\n",
      "Epoch 25 Batch 100 Loss 3.5860\n",
      "Epoch 25 Batch 150 Loss 3.5841\n",
      "Epoch 25 Loss 3.5855\n",
      "** on validation data loss is 3.5789\n",
      "Time taken for 1 epoch: 149.64 secs\n",
      "\n",
      "Saving checkpoint for epoch 25 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-25\n",
      "Epoch 26 Batch 0 Loss 3.6424\n",
      "Epoch 26 Batch 50 Loss 3.5814\n",
      "Epoch 26 Batch 100 Loss 3.5770\n",
      "Epoch 26 Batch 150 Loss 3.5804\n",
      "Epoch 26 Loss 3.5789\n",
      "** on validation data loss is 3.5614\n",
      "Time taken for 1 epoch: 150.25 secs\n",
      "\n",
      "Saving checkpoint for epoch 26 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-26\n",
      "Epoch 27 Batch 0 Loss 3.5416\n",
      "Epoch 27 Batch 50 Loss 3.5685\n",
      "Epoch 27 Batch 100 Loss 3.5699\n",
      "Epoch 27 Batch 150 Loss 3.5699\n",
      "Epoch 27 Loss 3.5728\n",
      "** on validation data loss is 3.5737\n",
      "Time taken for 1 epoch: 150.36 secs\n",
      "\n",
      "Saving checkpoint for epoch 27 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-27\n",
      "Epoch 28 Batch 0 Loss 3.5461\n",
      "Epoch 28 Batch 50 Loss 3.5621\n",
      "Epoch 28 Batch 100 Loss 3.5671\n",
      "Epoch 28 Batch 150 Loss 3.5698\n",
      "Epoch 28 Loss 3.5683\n",
      "** on validation data loss is 3.5505\n",
      "Time taken for 1 epoch: 150.25 secs\n",
      "\n",
      "Saving checkpoint for epoch 28 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-28\n",
      "Epoch 29 Batch 0 Loss 3.5147\n",
      "Epoch 29 Batch 50 Loss 3.5621\n",
      "Epoch 29 Batch 100 Loss 3.5637\n",
      "Epoch 29 Batch 150 Loss 3.5620\n",
      "Epoch 29 Loss 3.5646\n",
      "** on validation data loss is 3.5668\n",
      "Time taken for 1 epoch: 150.80 secs\n",
      "\n",
      "Saving checkpoint for epoch 29 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-29\n",
      "Epoch 30 Batch 0 Loss 3.5736\n",
      "Epoch 30 Batch 50 Loss 3.5533\n",
      "Epoch 30 Batch 100 Loss 3.5545\n",
      "Epoch 30 Batch 150 Loss 3.5574\n",
      "Epoch 30 Loss 3.5581\n",
      "** on validation data loss is 3.5832\n",
      "Time taken for 1 epoch: 151.77 secs\n",
      "\n",
      "Stopping early, last 2 val losses are: [<tf.Tensor: shape=(), dtype=float32, numpy=3.5668035>, <tf.Tensor: shape=(), dtype=float32, numpy=3.5831795>]                       \n",
      "Best was 3.551\n",
      "\n",
      "\n",
      "Wrote transformer.results to training_history/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64.pickle\n",
      "21:25\n",
      "Begin running v2b__nld_4-dm_128-nh_2-i_1-dr_0.1-opt_adam-lwi_0-bs_64\n",
      "Epoch 1 Batch 0 Loss 20.2133\n",
      "Epoch 1 Batch 50 Loss 11.2983\n",
      "Epoch 1 Batch 100 Loss 10.0723\n",
      "Epoch 1 Batch 150 Loss 9.4476\n",
      "Epoch 1 Loss 9.1697\n",
      "** on validation data loss is 7.4185\n",
      "Not recording acc: 'Transformer' object has no attribute 'acc_function'\n",
      "Time taken for 1 epoch: 151.38 secs\n",
      "\n",
      "Saving checkpoint for epoch 1 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-1\n",
      "Epoch 2 Batch 0 Loss 7.3497\n",
      "Epoch 2 Batch 50 Loss 7.3434\n",
      "Epoch 2 Batch 100 Loss 7.1863\n",
      "Epoch 2 Batch 150 Loss 7.0451\n",
      "Epoch 2 Loss 6.9767\n",
      "** on validation data loss is 6.4756\n",
      "Time taken for 1 epoch: 150.42 secs\n",
      "\n",
      "Saving checkpoint for epoch 2 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-2\n",
      "Epoch 3 Batch 0 Loss 6.4940\n",
      "Epoch 3 Batch 50 Loss 6.5196\n",
      "Epoch 3 Batch 100 Loss 6.4459\n",
      "Epoch 3 Batch 150 Loss 6.3928\n",
      "Epoch 3 Loss 6.3674\n",
      "** on validation data loss is 6.2575\n",
      "Time taken for 1 epoch: 150.61 secs\n",
      "\n",
      "Saving checkpoint for epoch 3 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-3\n",
      "Epoch 4 Batch 0 Loss 6.2115\n",
      "Epoch 4 Batch 50 Loss 6.1695\n",
      "Epoch 4 Batch 100 Loss 6.1177\n",
      "Epoch 4 Batch 150 Loss 6.0778\n",
      "Epoch 4 Loss 6.0711\n",
      "** on validation data loss is 5.9797\n",
      "Time taken for 1 epoch: 149.89 secs\n",
      "\n",
      "Saving checkpoint for epoch 4 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-4\n",
      "Epoch 5 Batch 0 Loss 6.1771\n",
      "Epoch 5 Batch 50 Loss 5.9353\n",
      "Epoch 5 Batch 100 Loss 5.8946\n",
      "Epoch 5 Batch 150 Loss 5.8502\n",
      "Epoch 5 Loss 5.8279\n",
      "** on validation data loss is 5.7240\n",
      "Time taken for 1 epoch: 150.30 secs\n",
      "\n",
      "Saving checkpoint for epoch 5 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-5\n",
      "Epoch 6 Batch 0 Loss 5.5740\n",
      "Epoch 6 Batch 50 Loss 5.6583\n",
      "Epoch 6 Batch 100 Loss 5.6274\n",
      "Epoch 6 Batch 150 Loss 5.5859\n",
      "Epoch 6 Loss 5.5566\n",
      "** on validation data loss is 5.3160\n",
      "Time taken for 1 epoch: 150.57 secs\n",
      "\n",
      "Saving checkpoint for epoch 6 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-6\n",
      "Epoch 7 Batch 0 Loss 5.3316\n",
      "Epoch 7 Batch 50 Loss 5.3118\n",
      "Epoch 7 Batch 100 Loss 5.2837\n",
      "Epoch 7 Batch 150 Loss 5.2484\n",
      "Epoch 7 Loss 5.2268\n",
      "** on validation data loss is 4.9964\n",
      "Time taken for 1 epoch: 150.32 secs\n",
      "\n",
      "Saving checkpoint for epoch 7 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-7\n",
      "Epoch 8 Batch 0 Loss 4.9730\n",
      "Epoch 8 Batch 50 Loss 5.0560\n",
      "Epoch 8 Batch 100 Loss 5.0349\n",
      "Epoch 8 Batch 150 Loss 4.9706\n",
      "Epoch 8 Loss 4.9373\n",
      "** on validation data loss is 4.8323\n",
      "Time taken for 1 epoch: 151.87 secs\n",
      "\n",
      "Saving checkpoint for epoch 8 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-8\n",
      "Epoch 9 Batch 0 Loss 4.8886\n",
      "Epoch 9 Batch 50 Loss 4.7295\n",
      "Epoch 9 Batch 100 Loss 4.6792\n",
      "Epoch 9 Batch 150 Loss 4.6481\n",
      "Epoch 9 Loss 4.6294\n",
      "** on validation data loss is 4.5116\n",
      "Time taken for 1 epoch: 152.56 secs\n",
      "\n",
      "Saving checkpoint for epoch 9 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-9\n",
      "Epoch 10 Batch 0 Loss 4.4895\n",
      "Epoch 10 Batch 50 Loss 4.4894\n",
      "Epoch 10 Batch 100 Loss 4.4586\n",
      "Epoch 10 Batch 150 Loss 4.4362\n",
      "Epoch 10 Loss 4.4194\n",
      "** on validation data loss is 4.3221\n",
      "Time taken for 1 epoch: 150.60 secs\n",
      "\n",
      "Saving checkpoint for epoch 10 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-10\n",
      "Epoch 11 Batch 0 Loss 4.3535\n",
      "Epoch 11 Batch 50 Loss 4.3014\n",
      "Epoch 11 Batch 100 Loss 4.2838\n",
      "Epoch 11 Batch 150 Loss 4.2697\n",
      "Epoch 11 Loss 4.2593\n",
      "** on validation data loss is 4.1384\n",
      "Time taken for 1 epoch: 149.64 secs\n",
      "\n",
      "Saving checkpoint for epoch 11 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-11\n",
      "Epoch 12 Batch 0 Loss 4.1413\n",
      "Epoch 12 Batch 50 Loss 4.1912\n",
      "Epoch 12 Batch 100 Loss 4.1651\n",
      "Epoch 12 Batch 150 Loss 4.1479\n",
      "Epoch 12 Loss 4.1396\n",
      "** on validation data loss is 4.0206\n",
      "Time taken for 1 epoch: 150.33 secs\n",
      "\n",
      "Saving checkpoint for epoch 12 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-12\n",
      "Epoch 13 Batch 0 Loss 4.0417\n",
      "Epoch 13 Batch 50 Loss 4.0679\n",
      "Epoch 13 Batch 100 Loss 4.0570\n",
      "Epoch 13 Batch 150 Loss 4.0516\n",
      "Epoch 13 Loss 4.0464\n",
      "** on validation data loss is 4.0450\n",
      "Time taken for 1 epoch: 150.84 secs\n",
      "\n",
      "Saving checkpoint for epoch 13 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-13\n",
      "Epoch 14 Batch 0 Loss 4.1018\n",
      "Epoch 14 Batch 50 Loss 3.9944\n",
      "Epoch 14 Batch 100 Loss 3.9874\n",
      "Epoch 14 Batch 150 Loss 3.9774\n",
      "Epoch 14 Loss 3.9741\n",
      "** on validation data loss is 3.9017\n",
      "Time taken for 1 epoch: 151.08 secs\n",
      "\n",
      "Saving checkpoint for epoch 14 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-14\n",
      "Epoch 15 Batch 0 Loss 3.9217\n",
      "Epoch 15 Batch 50 Loss 3.9349\n",
      "Epoch 15 Batch 100 Loss 3.9238\n",
      "Epoch 15 Batch 150 Loss 3.9195\n",
      "Epoch 15 Loss 3.9162\n",
      "** on validation data loss is 3.8990\n",
      "Time taken for 1 epoch: 151.43 secs\n",
      "\n",
      "Saving checkpoint for epoch 15 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-15\n",
      "Epoch 16 Batch 0 Loss 3.8772\n",
      "Epoch 16 Batch 50 Loss 3.8775\n",
      "Epoch 16 Batch 100 Loss 3.8807\n",
      "Epoch 16 Batch 150 Loss 3.8751\n",
      "Epoch 16 Loss 3.8716\n",
      "** on validation data loss is 3.8720\n",
      "Time taken for 1 epoch: 150.58 secs\n",
      "\n",
      "Saving checkpoint for epoch 16 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-16\n",
      "Epoch 17 Batch 0 Loss 3.9725\n",
      "Epoch 17 Batch 50 Loss 3.8396\n",
      "Epoch 17 Batch 100 Loss 3.8474\n",
      "Epoch 17 Batch 150 Loss 3.8425\n",
      "Epoch 17 Loss 3.8397\n",
      "** on validation data loss is 3.7875\n",
      "Time taken for 1 epoch: 149.90 secs\n",
      "\n",
      "Saving checkpoint for epoch 17 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-17\n",
      "Epoch 18 Batch 0 Loss 3.9186\n",
      "Epoch 18 Batch 50 Loss 3.8188\n",
      "Epoch 18 Batch 100 Loss 3.8087\n",
      "Epoch 18 Batch 150 Loss 3.8038\n",
      "Epoch 18 Loss 3.8036\n",
      "** on validation data loss is 3.8107\n",
      "Time taken for 1 epoch: 153.40 secs\n",
      "\n",
      "Saving checkpoint for epoch 18 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-18\n",
      "Epoch 19 Batch 0 Loss 3.8650\n",
      "Epoch 19 Batch 50 Loss 3.7814\n",
      "Epoch 19 Batch 100 Loss 3.7792\n",
      "Epoch 19 Batch 150 Loss 3.7765\n",
      "Epoch 19 Loss 3.7761\n",
      "** on validation data loss is 3.7487\n",
      "Time taken for 1 epoch: 157.45 secs\n",
      "\n",
      "Saving checkpoint for epoch 19 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-19\n",
      "Epoch 20 Batch 0 Loss 3.8778\n",
      "Epoch 20 Batch 50 Loss 3.7614\n",
      "Epoch 20 Batch 100 Loss 3.7614\n",
      "Epoch 20 Batch 150 Loss 3.7574\n",
      "Epoch 20 Loss 3.7531\n",
      "** on validation data loss is 3.7011\n",
      "Time taken for 1 epoch: 160.07 secs\n",
      "\n",
      "Saving checkpoint for epoch 20 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-20\n",
      "Epoch 21 Batch 0 Loss 3.7036\n",
      "Epoch 21 Batch 50 Loss 3.7421\n",
      "Epoch 21 Batch 100 Loss 3.7409\n",
      "Epoch 21 Batch 150 Loss 3.7351\n",
      "Epoch 21 Loss 3.7341\n",
      "** on validation data loss is 3.7336\n",
      "Time taken for 1 epoch: 160.72 secs\n",
      "\n",
      "Saving checkpoint for epoch 21 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-21\n",
      "Epoch 22 Batch 0 Loss 3.7166\n",
      "Epoch 22 Batch 50 Loss 3.7183\n",
      "Epoch 22 Batch 100 Loss 3.7187\n",
      "Epoch 22 Batch 150 Loss 3.7163\n",
      "Epoch 22 Loss 3.7149\n",
      "** on validation data loss is 3.6740\n",
      "Time taken for 1 epoch: 167.26 secs\n",
      "\n",
      "Saving checkpoint for epoch 22 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-22\n",
      "Epoch 23 Batch 0 Loss 3.7446\n",
      "Epoch 23 Batch 50 Loss 3.7057\n",
      "Epoch 23 Batch 100 Loss 3.7037\n",
      "Epoch 23 Batch 150 Loss 3.7009\n",
      "Epoch 23 Loss 3.6994\n",
      "** on validation data loss is 3.6569\n",
      "Time taken for 1 epoch: 176.34 secs\n",
      "\n",
      "Saving checkpoint for epoch 23 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-23\n",
      "Epoch 24 Batch 0 Loss 3.6400\n",
      "Epoch 24 Batch 50 Loss 3.6945\n",
      "Epoch 24 Batch 100 Loss 3.6903\n",
      "Epoch 24 Batch 150 Loss 3.6899\n",
      "Epoch 24 Loss 3.6871\n",
      "** on validation data loss is 3.6633\n",
      "Time taken for 1 epoch: 176.04 secs\n",
      "\n",
      "Saving checkpoint for epoch 24 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-24\n",
      "Epoch 25 Batch 0 Loss 3.6600\n",
      "Epoch 25 Batch 50 Loss 3.6807\n",
      "Epoch 25 Batch 100 Loss 3.6782\n",
      "Epoch 25 Batch 150 Loss 3.6770\n",
      "Epoch 25 Loss 3.6743\n",
      "** on validation data loss is 3.6746\n",
      "Time taken for 1 epoch: 176.35 secs\n",
      "\n",
      "Stopping early, last 2 val losses are: [<tf.Tensor: shape=(), dtype=float32, numpy=3.6632607>, <tf.Tensor: shape=(), dtype=float32, numpy=3.6745653>]                       \n",
      "Best was 3.657\n",
      "\n",
      "\n",
      "Wrote transformer.results to training_history/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64.pickle\n",
      "22:29\n",
      "Begin running v2b__nld_4-dm_128-nh_2-i_2-dr_0.1-opt_adam-lwi_0-bs_64\n",
      "Epoch 1 Batch 0 Loss 15.3978\n",
      "Epoch 1 Batch 50 Loss 10.3497\n",
      "Epoch 1 Batch 100 Loss 9.2862\n",
      "Epoch 1 Batch 150 Loss 8.7802\n",
      "Epoch 1 Loss 8.5680\n",
      "** on validation data loss is 7.3919\n",
      "Not recording acc: 'Transformer' object has no attribute 'acc_function'\n",
      "Time taken for 1 epoch: 178.49 secs\n",
      "\n",
      "Saving checkpoint for epoch 1 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-1\n",
      "Epoch 2 Batch 0 Loss 7.4776\n",
      "Epoch 2 Batch 50 Loss 7.2859\n",
      "Epoch 2 Batch 100 Loss 7.0895\n",
      "Epoch 2 Batch 150 Loss 6.9166\n",
      "Epoch 2 Loss 6.8245\n",
      "** on validation data loss is 6.1725\n",
      "Time taken for 1 epoch: 179.73 secs\n",
      "\n",
      "Saving checkpoint for epoch 2 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-2\n",
      "Epoch 3 Batch 0 Loss 6.1015\n",
      "Epoch 3 Batch 50 Loss 6.0484\n",
      "Epoch 3 Batch 100 Loss 5.8825\n",
      "Epoch 3 Batch 150 Loss 5.7430\n",
      "Epoch 3 Loss 5.6803\n",
      "** on validation data loss is 5.4688\n",
      "Time taken for 1 epoch: 178.88 secs\n",
      "\n",
      "Saving checkpoint for epoch 3 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-3\n",
      "Epoch 4 Batch 0 Loss 5.4431\n",
      "Epoch 4 Batch 50 Loss 5.1887\n",
      "Epoch 4 Batch 100 Loss 5.1221\n",
      "Epoch 4 Batch 150 Loss 5.0401\n",
      "Epoch 4 Loss 4.9953\n",
      "** on validation data loss is 4.7296\n",
      "Time taken for 1 epoch: 180.42 secs\n",
      "\n",
      "Saving checkpoint for epoch 4 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-4\n",
      "Epoch 5 Batch 0 Loss 4.6986\n",
      "Epoch 5 Batch 50 Loss 4.6494\n",
      "Epoch 5 Batch 100 Loss 4.6034\n",
      "Epoch 5 Batch 150 Loss 4.5648\n",
      "Epoch 5 Loss 4.5379\n",
      "** on validation data loss is 4.2645\n",
      "Time taken for 1 epoch: 181.52 secs\n",
      "\n",
      "Saving checkpoint for epoch 5 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-5\n",
      "Epoch 6 Batch 0 Loss 4.3193\n",
      "Epoch 6 Batch 50 Loss 4.3489\n",
      "Epoch 6 Batch 100 Loss 4.3215\n",
      "Epoch 6 Batch 150 Loss 4.2926\n",
      "Epoch 6 Loss 4.2804\n",
      "** on validation data loss is 4.1012\n",
      "Time taken for 1 epoch: 190.04 secs\n",
      "\n",
      "Saving checkpoint for epoch 6 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-6\n",
      "Epoch 7 Batch 0 Loss 4.1313\n",
      "Epoch 7 Batch 50 Loss 4.1626\n",
      "Epoch 7 Batch 100 Loss 4.1431\n",
      "Epoch 7 Batch 150 Loss 4.1258\n",
      "Epoch 7 Loss 4.1131\n",
      "** on validation data loss is 4.0515\n",
      "Time taken for 1 epoch: 195.75 secs\n",
      "\n",
      "Saving checkpoint for epoch 7 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-7\n",
      "Epoch 8 Batch 0 Loss 4.1251\n",
      "Epoch 8 Batch 50 Loss 4.0365\n",
      "Epoch 8 Batch 100 Loss 4.0185\n",
      "Epoch 8 Batch 150 Loss 4.0057\n",
      "Epoch 8 Loss 3.9951\n",
      "** on validation data loss is 3.9838\n",
      "Time taken for 1 epoch: 195.81 secs\n",
      "\n",
      "Saving checkpoint for epoch 8 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-8\n",
      "Epoch 9 Batch 0 Loss 4.0581\n",
      "Epoch 9 Batch 50 Loss 3.9399\n",
      "Epoch 9 Batch 100 Loss 3.9228\n",
      "Epoch 9 Batch 150 Loss 3.9122\n",
      "Epoch 9 Loss 3.9096\n",
      "** on validation data loss is 3.8489\n",
      "Time taken for 1 epoch: 188.99 secs\n",
      "\n",
      "Saving checkpoint for epoch 9 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-9\n",
      "Epoch 10 Batch 0 Loss 3.8491\n",
      "Epoch 10 Batch 50 Loss 3.8519\n",
      "Epoch 10 Batch 100 Loss 3.8509\n",
      "Epoch 10 Batch 150 Loss 3.8487\n",
      "Epoch 10 Loss 3.8453\n",
      "** on validation data loss is 3.7646\n",
      "Time taken for 1 epoch: 182.95 secs\n",
      "\n",
      "Saving checkpoint for epoch 10 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-10\n",
      "Epoch 11 Batch 0 Loss 3.8849\n",
      "Epoch 11 Batch 50 Loss 3.8134\n",
      "Epoch 11 Batch 100 Loss 3.8084\n",
      "Epoch 11 Batch 150 Loss 3.8028\n",
      "Epoch 11 Loss 3.7990\n",
      "** on validation data loss is 3.8029\n",
      "Time taken for 1 epoch: 180.77 secs\n",
      "\n",
      "Saving checkpoint for epoch 11 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-11\n",
      "Epoch 12 Batch 0 Loss 3.7606\n",
      "Epoch 12 Batch 50 Loss 3.7778\n",
      "Epoch 12 Batch 100 Loss 3.7688\n",
      "Epoch 12 Batch 150 Loss 3.7644\n",
      "Epoch 12 Loss 3.7631\n",
      "** on validation data loss is 3.7249\n",
      "Time taken for 1 epoch: 176.41 secs\n",
      "\n",
      "Saving checkpoint for epoch 12 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-12\n",
      "Epoch 13 Batch 0 Loss 3.7108\n",
      "Epoch 13 Batch 50 Loss 3.7381\n",
      "Epoch 13 Batch 100 Loss 3.7349\n",
      "Epoch 13 Batch 150 Loss 3.7359\n",
      "Epoch 13 Loss 3.7318\n",
      "** on validation data loss is 3.6889\n",
      "Time taken for 1 epoch: 176.40 secs\n",
      "\n",
      "Saving checkpoint for epoch 13 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-13\n",
      "Epoch 14 Batch 0 Loss 3.6990\n",
      "Epoch 14 Batch 50 Loss 3.7222\n",
      "Epoch 14 Batch 100 Loss 3.7165\n",
      "Epoch 14 Batch 150 Loss 3.7121\n",
      "Epoch 14 Loss 3.7099\n",
      "** on validation data loss is 3.7002\n",
      "Time taken for 1 epoch: 178.37 secs\n",
      "\n",
      "Saving checkpoint for epoch 14 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-14\n",
      "Epoch 15 Batch 0 Loss 3.7287\n",
      "Epoch 15 Batch 50 Loss 3.6919\n",
      "Epoch 15 Batch 100 Loss 3.6922\n",
      "Epoch 15 Batch 150 Loss 3.6894\n",
      "Epoch 15 Loss 3.6893\n",
      "** on validation data loss is 3.6507\n",
      "Time taken for 1 epoch: 175.81 secs\n",
      "\n",
      "Saving checkpoint for epoch 15 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-15\n",
      "Epoch 16 Batch 0 Loss 3.6796\n",
      "Epoch 16 Batch 50 Loss 3.6842\n",
      "Epoch 16 Batch 100 Loss 3.6764\n",
      "Epoch 16 Batch 150 Loss 3.6739\n",
      "Epoch 16 Loss 3.6724\n",
      "** on validation data loss is 3.6306\n",
      "Time taken for 1 epoch: 173.93 secs\n",
      "\n",
      "Saving checkpoint for epoch 16 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-16\n",
      "Epoch 17 Batch 0 Loss 3.6620\n",
      "Epoch 17 Batch 50 Loss 3.6587\n",
      "Epoch 17 Batch 100 Loss 3.6627\n",
      "Epoch 17 Batch 150 Loss 3.6603\n",
      "Epoch 17 Loss 3.6588\n",
      "** on validation data loss is 3.6449\n",
      "Time taken for 1 epoch: 173.11 secs\n",
      "\n",
      "Saving checkpoint for epoch 17 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64/ckpt-17\n",
      "Epoch 18 Batch 0 Loss 3.7013\n",
      "Epoch 18 Batch 50 Loss 3.6451\n",
      "Epoch 18 Batch 100 Loss 3.6473\n",
      "Epoch 18 Batch 150 Loss 3.6447\n",
      "Epoch 18 Loss 3.6464\n",
      "** on validation data loss is 3.6472\n",
      "Time taken for 1 epoch: 175.41 secs\n",
      "\n",
      "Stopping early, last 2 val losses are: [<tf.Tensor: shape=(), dtype=float32, numpy=3.6448927>, <tf.Tensor: shape=(), dtype=float32, numpy=3.647176>]                       \n",
      "Best was 3.631\n",
      "\n",
      "\n",
      "Wrote transformer.results to training_history/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64.pickle\n"
     ]
    }
   ],
   "source": [
    "from my_lib.BanksformerGen import Transformer\n",
    "import pickle \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_models = []\n",
    "for_df = []\n",
    "\n",
    "\n",
    "def to_num(x):\n",
    "    try: return int(x)\n",
    "    except: return float(x)\n",
    "\n",
    "def id_str_to_folder(id_str):\n",
    "    return id_str.replace(\".\", \"__\")\n",
    "beta = 1\n",
    "\n",
    "\n",
    "# moredate\n",
    "LOSS_WEIGHTS_OLD = {\n",
    " 'td_sc':1.,\n",
    " 'year': 0.5,\n",
    " 'month': 0.15,\n",
    " 'day': 0.25,\n",
    " 'dow': 0.1,\n",
    " 'tcode_num': 1.,\n",
    " 'log_amount_sc': 2.}\n",
    "\n",
    "\n",
    "LOSS_WEIGHTS_0 = {\n",
    " 'td_sc':1.,\n",
    " 'month': 0.015,\n",
    " 'day': 0.025,\n",
    " 'dow': 0.01,\n",
    " 'tcode_num': 1.,\n",
    " 'log_amount_sc': 2.}\n",
    "\n",
    "\n",
    "\n",
    "LOSS_WEIGHTS_MID = {\n",
    " 'td_sc':1.,\n",
    " 'month': 0.07,\n",
    " 'day': 0.1,\n",
    " 'dow': 0.04,\n",
    " 'tcode_num': 1.,\n",
    " 'log_amount_sc': 2.}\n",
    "\n",
    "\n",
    "\n",
    "lws = [(LOSS_WEIGHTS_0, \"0\"), (LOSS_WEIGHTS_OLD, \"moredate\")]\n",
    "\n",
    "# lws = [(LOSS_WEIGHTS_MID, \"mid\")]\n",
    "\n",
    "td_loss_fns = [(loss_mse, \"loss_mse\")]\n",
    "\n",
    "\n",
    "EPOCHS = 80\n",
    "EARLY_STOP = 2\n",
    "\n",
    "num_layers_enc = None\n",
    "dropout_rate = 0.1\n",
    "dr = dropout_rate\n",
    "opt_name = \"adam\"\n",
    "# td_loss_fn = loss_mse\n",
    "\n",
    "\n",
    "## Tuned these ! \n",
    "d_model = 128\n",
    "num_layers_dec = 4\n",
    "num_heads = 2\n",
    "bs = 64\n",
    "# lws # above\n",
    "\n",
    "\n",
    "LOSS_WEIGHTS, lwi = lws[0]\n",
    "\n",
    "\n",
    "### newer fields, copy loss weights from similar fields\n",
    "LOSS_WEIGHTS[\"dtme\"] = LOSS_WEIGHTS[\"day\"]\n",
    "LOSS_WEIGHTS[\"k_symbol_num\"] = LOSS_WEIGHTS[\"tcode_num\"]\n",
    "LOSS_WEIGHTS[\"operation_num\"] = LOSS_WEIGHTS[\"tcode_num\"]\n",
    "LOSS_WEIGHTS[\"type_num\"] = LOSS_WEIGHTS[\"tcode_num\"]\n",
    "\n",
    "\n",
    "dff = d_model\n",
    "\n",
    "\n",
    "# skip_next = True   \n",
    "for i in range(3):\n",
    "\n",
    "#     for num_layers_dec in [3]:\n",
    "#         for d_model in [32, 128]:\n",
    "#             for num_heads in [2,4]:\n",
    "            \n",
    "                # v\n",
    "\n",
    "                start = time.time()\n",
    "\n",
    "\n",
    "                print(datetime.datetime.now().strftime(\"%H:%M\"))\n",
    "\n",
    "\n",
    "                transformer = Transformer(\n",
    "                    num_layers_enc=num_layers_enc, num_layers_dec=num_layers_dec,\n",
    "                    d_model=d_model,\n",
    "                    num_heads=num_heads,\n",
    "                    dff=dff,\n",
    "                    maximum_position_encoding=256,\n",
    "                   net_info = None, \n",
    "                    inp_dim = n_feat_inp,\n",
    "                    final_dim= None,\n",
    "                    config=config,\n",
    "                    rate=dr)\n",
    "\n",
    "                optimizer = tf.keras.optimizers.Adam()\n",
    "                transformer.optimizer =  optimizer\n",
    "\n",
    "\n",
    "                train_batches = make_batches(ds_tr, BUFFER_SIZE, bs)\n",
    "\n",
    "\n",
    "                transformer.loss_function = loss_function\n",
    "\n",
    "                transformer.LOSS_WEIGHTS = LOSS_WEIGHTS\n",
    "\n",
    "                id_str = f\"v2b__nld_{num_layers_dec}-dm_{d_model}-nh_{num_heads}-i_{i}-dr_{dr}-opt_{opt_name}-lwi_{lwi}-bs_{bs}\"\n",
    "\n",
    "                print(\"Begin running\", id_str)\n",
    "                transformer.id_str = id_str\n",
    "\n",
    "\n",
    "                all_models.append(transformer)\n",
    "                transformer.compile()\n",
    "\n",
    "\n",
    "                transformer.checkpoint_path = f\"./checkpoints/{id_str_to_folder(transformer.id_str)}\"\n",
    "                transformer.ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                                           optimizer=optimizer)\n",
    "                transformer.ckpt_manager = tf.train.CheckpointManager(transformer.ckpt, \n",
    "                                                                      transformer.checkpoint_path, max_to_keep=EARLY_STOP)\n",
    "\n",
    "                \n",
    "                \n",
    "                ## If network already exists, load weights instead of training\n",
    "                if transformer.ckpt_manager.latest_checkpoint:\n",
    "                    transformer.ckpt.restore(transformer.ckpt_manager.latest_checkpoint)\n",
    "                    print('Latest checkpoint restored!!')    \n",
    "                    continue\n",
    "\n",
    "\n",
    "                transformer.fit(train_batches, inp_tensor_cv, tar_tensor_cv, \n",
    "                                epochs= EPOCHS, early_stop=EARLY_STOP, print_every=50, ckpt_every = 1)\n",
    "\n",
    "                transformer.fit_time = time.time() - start\n",
    "                transformer.results[\"fit_time\"] = transformer.fit_time \n",
    "\n",
    "                with open(f\"training_history/{id_str_to_folder(transformer.id_str)}.pickle\", \"wb\") as f:\n",
    "                    pickle.dump(transformer.results, f) \n",
    "                    print(\"Wrote transformer.results to\", f.name)\n",
    "\n",
    "\n",
    "                for_df.append((num_layers_dec, d_model, num_heads, i, dr, beta, dff,\n",
    "                               np.min(transformer.results[\"val_loss\"]), opt_name, transformer.id_str))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(for_df, columns=['num_layers_dec', 'd_model', 'num_heads', 'i', \"dr\", \"beta\", \"dff\",\n",
    "                                                \"val loss\", \"opt name\",\"id_str\"]).sort_values(\"val loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_layers_dec</th>\n",
       "      <th>d_model</th>\n",
       "      <th>num_heads</th>\n",
       "      <th>i</th>\n",
       "      <th>dr</th>\n",
       "      <th>beta</th>\n",
       "      <th>dff</th>\n",
       "      <th>val loss</th>\n",
       "      <th>opt name</th>\n",
       "      <th>id_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>3.550501</td>\n",
       "      <td>adam</td>\n",
       "      <td>v2b__nld_4-dm_128-nh_2-i_0-dr_0.1-opt_adam-lwi_0-bs_64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>3.630562</td>\n",
       "      <td>adam</td>\n",
       "      <td>v2b__nld_4-dm_128-nh_2-i_2-dr_0.1-opt_adam-lwi_0-bs_64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>3.656870</td>\n",
       "      <td>adam</td>\n",
       "      <td>v2b__nld_4-dm_128-nh_2-i_1-dr_0.1-opt_adam-lwi_0-bs_64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_layers_dec  d_model  num_heads  i   dr  beta  dff  val loss opt name  \\\n",
       "0               4      128          2  0  0.1     1  128  3.550501     adam   \n",
       "2               4      128          2  2  0.1     1  128  3.630562     adam   \n",
       "1               4      128          2  1  0.1     1  128  3.656870     adam   \n",
       "\n",
       "                                                   id_str  \n",
       "0  v2b__nld_4-dm_128-nh_2-i_0-dr_0.1-opt_adam-lwi_0-bs_64  \n",
       "2  v2b__nld_4-dm_128-nh_2-i_2-dr_0.1-opt_adam-lwi_0-bs_64  \n",
       "1  v2b__nld_4-dm_128-nh_2-i_1-dr_0.1-opt_adam-lwi_0-bs_64  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', None, \"display.max_rows\", None, \"display.max_columns\", None):\n",
    "    display(df.sort_values(\"val loss\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_layers_dec</th>\n",
       "      <th>d_model</th>\n",
       "      <th>num_heads</th>\n",
       "      <th>i</th>\n",
       "      <th>dr</th>\n",
       "      <th>beta</th>\n",
       "      <th>dff</th>\n",
       "      <th>val loss</th>\n",
       "      <th>opt name</th>\n",
       "      <th>id_str</th>\n",
       "      <th>bs</th>\n",
       "      <th>lwi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>3.550501</td>\n",
       "      <td>adam</td>\n",
       "      <td>v2b__nld_4-dm_128-nh_2-i_0-dr_0.1-opt_adam-lwi_0-bs_64</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>3.630562</td>\n",
       "      <td>adam</td>\n",
       "      <td>v2b__nld_4-dm_128-nh_2-i_2-dr_0.1-opt_adam-lwi_0-bs_64</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>3.656870</td>\n",
       "      <td>adam</td>\n",
       "      <td>v2b__nld_4-dm_128-nh_2-i_1-dr_0.1-opt_adam-lwi_0-bs_64</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_layers_dec  d_model  num_heads  i   dr  beta  dff  val loss opt name  \\\n",
       "0               4      128          2  0  0.1     1  128  3.550501     adam   \n",
       "2               4      128          2  2  0.1     1  128  3.630562     adam   \n",
       "1               4      128          2  1  0.1     1  128  3.656870     adam   \n",
       "\n",
       "                                                   id_str  bs lwi  \n",
       "0  v2b__nld_4-dm_128-nh_2-i_0-dr_0.1-opt_adam-lwi_0-bs_64  64   0  \n",
       "2  v2b__nld_4-dm_128-nh_2-i_2-dr_0.1-opt_adam-lwi_0-bs_64  64   0  \n",
       "1  v2b__nld_4-dm_128-nh_2-i_1-dr_0.1-opt_adam-lwi_0-bs_64  64   0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try: # will fail if loaded weight and not trained\n",
    "    \n",
    "    bs = df.id_str.apply(lambda x: int(x.split(\"-\")[-1].split(\"_\")[-1]))\n",
    "    lwi = df.id_str.apply(lambda x: x.split(\"-\")[-2].split(\"_\")[-1])\n",
    "    \n",
    "\n",
    "    df['bs'] = bs\n",
    "    df['lwi'] = lwi\n",
    "    \n",
    "\n",
    "\n",
    "    with pd.option_context('display.max_colwidth', None, \"display.max_rows\", None, \"display.max_columns\", None):\n",
    "        display(df.sort_values([\"lwi\", \"val loss\"]))\n",
    "        \n",
    "        \n",
    "    times= [t.fit_time for t in all_models]\n",
    "    df['times'] = times\n",
    "    \n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Failed with error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = all_models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.529211 , 6.3388853, 5.803706 , 4.713489 , 4.2920885, 4.268908 ,\n",
       "       3.9633422, 3.87718  , 3.8371792, 3.8153808, 3.7553988, 3.6720362,\n",
       "       3.680633 , 3.6664991, 3.6511633, 3.6401076, 3.6226833, 3.6165934,\n",
       "       3.6053796, 3.6089058, 3.5888238, 3.5830383, 3.575469 , 3.5710444,\n",
       "       3.578946 , 3.5613787, 3.573672 , 3.5505006, 3.5668035, 3.5831795],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(transformer.results[\"val_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f93d5fcd3d0>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.array(transformer.results[\"val_loss\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate  \n",
    "Warning: Code below is not nice and should be refactored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_YEARS_SPAN = 15   # will throw an error if it tries to generate dates this far past data_encoder.START_DATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running clocks[7] = np.array([encode_time_value(val, 7) for val in range(7)])\n",
      "Running clocks[31] = np.array([encode_time_value(val, 31) for val in range(31)])\n",
      "Running clocks[12] = np.array([encode_time_value(val, 12) for val in range(12)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys([7, 31, 12])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from my_lib.encoding import encode_time_value\n",
    "#, decode_time_value\n",
    "\n",
    "clocks = {}\n",
    "for max_val in [7, 31, 12]:\n",
    "    cmd = f\"clocks[{max_val}] = np.array([encode_time_value(val, {max_val}) for val in range({max_val})])\"\n",
    "    print(\"Running\", cmd)\n",
    "    exec(cmd)\n",
    "    \n",
    "clocks.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START_DATE = data_encoder.START_DATE \n",
    "START_DATE = str(data_encoder.START_DATE).split()[0]\n",
    "\n",
    "import calendar\n",
    "get_dtme = lambda d: calendar.monthrange(d.year, d.month)[1] - d.day\n",
    "\n",
    "if type(START_DATE) == str:\n",
    "    START_DATE = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\").date()\n",
    "    \n",
    "    \n",
    "\n",
    "END_DATE = START_DATE.replace(year = START_DATE.year+ MAX_YEARS_SPAN)\n",
    "\n",
    "ALL_DATES = [START_DATE + datetime.timedelta(i) for i in range((END_DATE - START_DATE).days)]\n",
    "\n",
    "\n",
    "# Keep all date information in an array \n",
    "# when generating dates, I keep track of dates with an offset from data_encoder.START_DATE\n",
    "# when generating a batch of dates can get the info as AD[offset_inds,:]\n",
    "AD = np.array([(d.month % 12, d.day % 31, d.weekday() % 7, i, d.year, get_dtme(d)) for i, d in enumerate(ALL_DATES)])\n",
    "\n",
    "AD[[0,3], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_lib.transformer_core import create_masks\n",
    "from my_lib.encoding import bulk_encode_time_value\n",
    "    \n",
    "    \n",
    "# This function is used when generating new samples\n",
    "# it takes the raw predictions output by the layer 'net_name' which corrisponds to a data field.\n",
    "# The predictions encode a probablity distribution, and here we sample the appropiate distribution\n",
    "# and reencodes the samples to the appropriate input format.\n",
    "def reencode_net_prediction(net_name, predictions):\n",
    "    \n",
    "    # print(\"reencode_net_prediction:\", net_name, predictions.shape)\n",
    "    \n",
    "    continous_fields = ['td_sc', \"log_amount_sc\"]\n",
    "    cat_fields = data_encoder.field_info[\"CAT_FIELDS\"]\n",
    "    date_fields = ['month', 'day', 'dtme', 'dow']\n",
    "    \n",
    "    batch_size = predictions.shape[0]\n",
    "    \n",
    "\n",
    "    # Continous values fields (bs, len, 2), first feature is mean, second is logvar for normal distribtion\n",
    "    #    need to return shape (bs, len, 1) \n",
    "    if net_name in continous_fields:\n",
    "        mean, log_var = predictions[:, :, 0:1],  predictions[:, :, 1:2]\n",
    "        # sd = np.sqrt(np.exp(log_var))\n",
    "        log_sd = log_var/2.\n",
    "        return mean +  log_sd*np.random.normal(size=(batch_size, 1, 1)) \n",
    "\n",
    "    \n",
    "    # Dates are represented as categorical distribution, \n",
    "    # and need to be reencoded with the clock encoding\n",
    "    elif net_name in date_fields:\n",
    "        dim = FIELD_DIMS_NET[net_name]\n",
    "        choices = np.arange(dim)\n",
    "        ps = tf.nn.softmax(predictions, axis=2).numpy().reshape(-1, dim)\n",
    "        \n",
    "        choosen =  np.array([np.random.choice(choices, p=p) for p in ps])\n",
    "\n",
    "        x = bulk_encode_time_value(choosen, max_val=dim)\n",
    "                \n",
    "        return np.reshape(x, newshape=(batch_size, -1, 2))\n",
    "        \n",
    "\n",
    "    \n",
    "    # Categorical variables are categorical distributions,\n",
    "    # and need to be reencoded with one hot encoding\n",
    "    elif net_name in cat_fields:\n",
    "    # elif \"_num\" in net_name:\n",
    "        dim = FIELD_DIMS_NET[net_name]\n",
    "        choices = np.arange(dim)\n",
    "\n",
    "        ps = tf.nn.softmax(predictions, axis=2).numpy().reshape(-1, dim)        \n",
    "        choosen =  np.reshape([np.random.choice(choices, p=p) for p in ps], newshape=(batch_size, -1))\n",
    "\n",
    "        return tf.one_hot(choosen, depth=dim)\n",
    "    \n",
    "    else:\n",
    "        raise Exception(f\"Got invalid net_name: {net_name}\")\n",
    "\n",
    "        \n",
    "days_per_month = np.array([(datetime.date(1990, month, 1) - datetime.timedelta(1)).day for month in range(1,13)]) # 0 = dec\n",
    "\n",
    "\n",
    "@np.vectorize\n",
    "def get_short_name(tcode):\n",
    "    return short_names[tcode]\n",
    "\n",
    "\n",
    "\n",
    "# Decodes the raw tensor output from Banksformer to data frame with proper scaling and encoding \n",
    "def bulk_decode(seqs, start_dates, return_single_df=False, return_df_list=False):\n",
    "    \n",
    "    # *****\n",
    "    ages = seqs[:, 0, :] * data_encoder.ATTR_SCALE\n",
    "    seqs = seqs[:, 1:, :]\n",
    "    assert np.sum(np.diff(ages)) == 0, f\"Bad formating, expected all entries same in each row, got {ages}\"\n",
    "\n",
    "    \n",
    "    amts = seqs[:, :, FIELD_STARTS_IN[\"log_amount_sc\"]].numpy() * data_encoder.LOG_AMOUNT_SCALE\n",
    "    amts = 10 ** amts\n",
    "    amts = np.round(amts - 1.0, 2)\n",
    "\n",
    "\n",
    "    days_passed = np.round(seqs[:, :, FIELD_STARTS_IN[\"td_sc\"]] *data_encoder.TD_SCALE ).astype(int)\n",
    "  \n",
    "\n",
    "#     years = np.round(seqs[:, :, FIELD_STARTS[\"year\"]]/ YEAR_SCALE).astype(int) + START_YEAR\n",
    "\n",
    "    months = np.argmax(seqs[:, :, FIELD_STARTS_IN[\"month\"]: FIELD_STARTS_IN[\"month\"] + FIELD_DIMS_IN[\"month\"]], axis=-1)\n",
    "    \n",
    "    \n",
    "    days = np.argmax(seqs[:, :, FIELD_STARTS_IN[\"day\"]: FIELD_STARTS_IN[\"day\"] + FIELD_DIMS_IN[\"day\"]], axis=-1)\n",
    "    days[days==0] = days_per_month[months[days==0]]\n",
    "    months[months==0] = 12 # needs to be done after days (above)\n",
    "    # date_fields = get_date_str(months, days)\n",
    "    \n",
    "    dpc = np.cumsum(days_passed, axis=1) \n",
    "    dates = np.array([[start_dates[i] + datetime.timedelta(int(d)) for d in dpc[i]]for i in range(len(start_dates))])\n",
    "    \n",
    "    # numbers\n",
    "    code_col_names = []\n",
    "    code_vals = [] \n",
    "    \n",
    "    # code names\n",
    "    code_names = []\n",
    "    code_names_cols = []\n",
    "    for field, start_i in FIELD_STARTS_IN.items():\n",
    "        if \"_num\" in field:\n",
    "            code_col_names.append(field)\n",
    "            code_vals.append(np.argmax(seqs[:, :, start_i: start_i + FIELD_DIMS_IN[field]], axis=-1))\n",
    "            \n",
    "            \n",
    "\n",
    "    # add customer age to all transactions\n",
    "    ages = np.repeat(ages[:, 0:1], amts.shape[1], axis=1).astype(int) \n",
    "    \n",
    "\n",
    "    return_vals = amts, *code_vals, *code_names, days_passed, ages, dates\n",
    "    return_lbls = \"amount\", *code_col_names, *code_names_cols, \"days_passed\", \"age\", \"date\"\n",
    "\n",
    "    if return_df_list:\n",
    "        return [add_code_names(pd.DataFrame.from_records(zip(*x), columns=return_lbls)) \n",
    "                    for x in zip(*return_vals)]\n",
    "    \n",
    "    if return_single_df:\n",
    "        df = pd.DataFrame.from_records([x for x in zip(*[x.reshape(-1) for x in return_vals])], columns=return_lbls)\n",
    "        return add_code_names(df)\n",
    "    \n",
    "    \n",
    "    return return_vals\n",
    "\n",
    "\n",
    "\n",
    "def add_code_names(df):\n",
    "    for field in CAT_FIELDS:\n",
    "        field = field.replace(\"_num\", \"\")\n",
    "        df[field] = df[field + \"_num\"].apply(lambda x: get_code_from_num(data_encoder, field, x))\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# returns the number corrisponding to 'code', which is an option for 'field'\n",
    "def get_code_num(data_encoder, field, code):\n",
    "    field = field.replace(\"_num\", \"\")\n",
    "    d = data_encoder.__getattribute__(f\"{field}_to_num\".upper())\n",
    "    return d[code]\n",
    "\n",
    "\n",
    "# returns the code corrisponding to the number 'num', which is an option for 'field'\n",
    "def get_code_from_num(data_encoder, field, num):\n",
    "    field = field.replace(\"_num\", \"\")\n",
    "    d = data_encoder.__getattribute__(f\"num_to_{field}\".upper())\n",
    "    return d[num]\n",
    "\n",
    "CAT_FIELDS = data_encoder.field_info[\"CAT_FIELDS\"]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seqs(length, ages, start_dates, greedy_dates = False, return_single_df=False, return_df_list=False):\n",
    "    \n",
    "    if return_single_df and return_df_list:\n",
    "        raise Exception(\"At most one of: 'return_single_df' and 'return_df_list' can be true\")\n",
    "    \n",
    "    date_inds = np.array([(d - START_DATE).days for d in start_dates])\n",
    "    \n",
    "    max_length = length\n",
    "\n",
    "    output = np.repeat(np.array(ages)[:, None, None], repeats=n_feat_inp, axis=2) / data_encoder.ATTR_SCALE\n",
    "    \n",
    "    raw_preds = []\n",
    "    raw_preds.append(output)\n",
    "\n",
    "    date_info = None\n",
    "    \n",
    "    \n",
    "    for i in range(max_length):\n",
    "\n",
    "\n",
    "        combined_mask, dec_padding_mask = create_masks(output)\n",
    "\n",
    "        \n",
    "        enc_preds, attn, raw_ps, date_inds  = call_to_generate(transformer, output, \n",
    "                                                 True, \n",
    "                                                 combined_mask, \n",
    "                                                 dec_padding_mask, date_inds, date_info, greedy_dates =greedy_dates)\n",
    "\n",
    "        \n",
    "        raw_preds.append(raw_ps)\n",
    "\n",
    "        enc_preds = tf.reshape(tf.constant(enc_preds), shape=(-1,1, n_feat_inp))\n",
    "\n",
    "        output = tf.concat([output, enc_preds], axis=1)\n",
    "\n",
    "        \n",
    "    return bulk_decode(output, start_dates, return_single_df, return_df_list), output, raw_preds\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Forward pass through transformer\n",
    "# \n",
    "# Returns: preds, attn_w, raw_preds, inds\n",
    "# the returned preds have multiple timesteps, but we only \n",
    "# care about the last (it's the only new one)\n",
    "def call_to_generate(transformer, tar, training,\n",
    "           look_ahead_mask, dec_padding_mask, start_inds, prev_date_info=None, greedy_dates = False):\n",
    "    \n",
    "\n",
    "    ### Pass through decoder stack ###\n",
    "    dec_output, attention_weights = transformer.decoder(\n",
    "        tar, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "\n",
    "    final_output = transformer.final_layer(dec_output) \n",
    "\n",
    "    \n",
    "    \n",
    "    ### Predict each field  ###\n",
    "    preds = {}\n",
    "    raw_preds = {}\n",
    "    encoded_preds_d = {}\n",
    "    \n",
    "\n",
    "\n",
    "    for net_name in transformer.ORDER:  \n",
    "\n",
    "        pred = transformer.__getattribute__(net_name)(final_output)\n",
    "        raw_preds[net_name] = pred\n",
    "        \n",
    "        # print(\"pre reencode_net_prediction:\", net_name, pred.shape)\n",
    "        pred = reencode_net_prediction(net_name, pred) # keeps time step\n",
    "        # print(\"post reencode_net_prediction:\", net_name, pred.shape)\n",
    "\n",
    "        encoded_preds_d[net_name] = pred[:,-1,:] \n",
    "\n",
    "        final_output = tf.concat([final_output, pred], axis=2)\n",
    "            \n",
    "    \n",
    "    combined_date_info, inds = raw_dates_to_reencoded(raw_preds, start_inds)\n",
    "    \n",
    "    encoded_preds_d.update(combined_date_info)\n",
    "\n",
    "    l = [encoded_preds_d[k] for k in transformer.ORDER]\n",
    "    encoded_preds =  tf.expand_dims(tf.concat(l, axis=1), axis=1)\n",
    "    \n",
    "    \n",
    "\n",
    "    return encoded_preds, attention_weights, raw_preds, start_inds + inds, \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "PMF_EPS = 1e-6\n",
    "\n",
    "# Takes raw predictions (info about predicted day, month, dow, and days passed) and start inds \n",
    "# (indicate the current date for each of the seqs) \n",
    "# Computes a number of days passed for each based on inputs (either greedily or with sampling)\n",
    "# returns the new_dates (old_dates + days passed) and their indicies\n",
    "def raw_dates_to_reencoded(raw, start_inds,  max_days = 100, greedy_decode=False):\n",
    "    \n",
    "    all_ps = [tf.nn.softmax(raw[k][:,-1]).numpy() for k in [\"month\", \"day\", \"dow\", \"dtme\"]]\n",
    "\n",
    "    timesteps = np.zeros(len(start_inds)).astype(int)\n",
    "\n",
    "    sc = data_encoder.TD_SCALE\n",
    "    for i, (month_ps, day_ps, dow_ps, dtme_ps, td_pred, si) in enumerate(zip(*all_ps, raw[\"td_sc\"][:,-1].numpy(), start_inds)):\n",
    "        \n",
    "        \n",
    "\n",
    "        ps = month_ps[AD[si:si+max_days,0]]*day_ps[AD[si:si+max_days,1]]*dow_ps[AD[si:si+max_days,2]] *dtme_ps[AD[si:si+max_days,-1]] * \\\n",
    "                np.exp(log_normal_pdf_gen(AD[si:si+max_days,3]-si, mean = td_pred[0]*sc, logvar=td_pred[1]*sc))\n",
    "#                 pmf(max(PMF_EPS, l_pred)*data_encoder.TD_SCALE, AD[si:si+max_days,3]-si ) \n",
    "\n",
    "        \n",
    "        if greedy_decode:\n",
    "            timesteps[i] = np.argmax(ps)\n",
    "        else:\n",
    "            timesteps[i] = np.random.choice(max_days, p=ps/sum(ps))\n",
    "        \n",
    "        \n",
    "    inds = start_inds + timesteps\n",
    "    \n",
    "    \n",
    "    return_ = {}\n",
    "    return_[\"td_sc\"] = tf.expand_dims(timesteps.astype(np.float32)/ data_encoder.TD_SCALE, axis=1)\n",
    "    return_[\"month\"] = bulk_encode_time_value(AD[inds, 0], 12)\n",
    "    return_[\"day\"] = bulk_encode_time_value(AD[inds, 1], 31)\n",
    "    return_[\"dow\"] = bulk_encode_time_value(AD[inds, 2], 7)\n",
    "    return_[\"dtme\"] = bulk_encode_time_value(AD[inds, -1], 31)\n",
    "    \n",
    "    \n",
    "\n",
    "    return return_, timesteps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs_dfs, seqs, raw = generate_seqs(length= 25, \n",
    "                          ages=[75, 25], \n",
    "                          start_dates=[START_DATE, START_DATE+datetime.timedelta(days=1)], \n",
    "                          greedy_dates=False,\n",
    "                          return_df_list=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amount</th>\n",
       "      <th>k_symbol_num</th>\n",
       "      <th>operation_num</th>\n",
       "      <th>type_num</th>\n",
       "      <th>days_passed</th>\n",
       "      <th>age</th>\n",
       "      <th>date</th>\n",
       "      <th>k_symbol</th>\n",
       "      <th>operation</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1204.630005</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-01-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CASH WITHDRAWAL</td>\n",
       "      <td>DEBIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1608.829956</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-01-08</td>\n",
       "      <td>HOUSEHOLD</td>\n",
       "      <td>REMITTANCE TO ANOTHER BANK</td>\n",
       "      <td>DEBIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3956.620117</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-01-11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CREDIT IN CASH</td>\n",
       "      <td>CREDIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1418.010010</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-01-11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CASH WITHDRAWAL</td>\n",
       "      <td>DEBIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1856.709961</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-01-11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CREDIT IN CASH</td>\n",
       "      <td>CREDIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27.730000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-01-31</td>\n",
       "      <td>INTEREST CREDITED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CREDIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9.160000</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-01-31</td>\n",
       "      <td>PAYMENT ON STATEMENT</td>\n",
       "      <td>CASH WITHDRAWAL</td>\n",
       "      <td>DEBIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1005.950012</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-02-09</td>\n",
       "      <td>HOUSEHOLD</td>\n",
       "      <td>REMITTANCE TO ANOTHER BANK</td>\n",
       "      <td>DEBIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3604.959961</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-02-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CREDIT IN CASH</td>\n",
       "      <td>CREDIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1582.949951</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-02-20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CASH WITHDRAWAL</td>\n",
       "      <td>DEBIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>57.799999</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-02-28</td>\n",
       "      <td>INTEREST CREDITED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CREDIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>18.070000</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-02-28</td>\n",
       "      <td>PAYMENT ON STATEMENT</td>\n",
       "      <td>CASH WITHDRAWAL</td>\n",
       "      <td>DEBIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1443.989990</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-03-09</td>\n",
       "      <td>HOUSEHOLD</td>\n",
       "      <td>REMITTANCE TO ANOTHER BANK</td>\n",
       "      <td>DEBIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4952.450195</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-03-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CREDIT IN CASH</td>\n",
       "      <td>CREDIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>56.400002</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-03-31</td>\n",
       "      <td>INTEREST CREDITED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CREDIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>11.420000</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-03-31</td>\n",
       "      <td>PAYMENT ON STATEMENT</td>\n",
       "      <td>CASH WITHDRAWAL</td>\n",
       "      <td>DEBIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1414.770020</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-04-09</td>\n",
       "      <td>HOUSEHOLD</td>\n",
       "      <td>REMITTANCE TO ANOTHER BANK</td>\n",
       "      <td>DEBIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4271.930176</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-04-09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CREDIT IN CASH</td>\n",
       "      <td>CREDIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>60.750000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-04-30</td>\n",
       "      <td>INTEREST CREDITED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CREDIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13.480000</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-04-30</td>\n",
       "      <td>PAYMENT ON STATEMENT</td>\n",
       "      <td>CASH WITHDRAWAL</td>\n",
       "      <td>DEBIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1723.959961</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-05-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CASH WITHDRAWAL</td>\n",
       "      <td>DEBIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2142.770020</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-05-11</td>\n",
       "      <td>HOUSEHOLD</td>\n",
       "      <td>REMITTANCE TO ANOTHER BANK</td>\n",
       "      <td>DEBIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4821.609863</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-05-11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CREDIT IN CASH</td>\n",
       "      <td>CREDIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1687.869995</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-05-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CASH WITHDRAWAL</td>\n",
       "      <td>DEBIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>66.389999</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-05-31</td>\n",
       "      <td>INTEREST CREDITED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CREDIT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         amount  k_symbol_num  operation_num  type_num  days_passed  age  \\\n",
       "0   1204.630005             0              3         1            1   25   \n",
       "1   1608.829956             3              4         1            5   25   \n",
       "2   3956.620117             0              0         0            3   25   \n",
       "3   1418.010010             0              3         1            0   25   \n",
       "4   1856.709961             0              0         0            0   25   \n",
       "5     27.730000             1              2         0           20   25   \n",
       "6      9.160000             2              3         1            0   25   \n",
       "7   1005.950012             3              4         1            9   25   \n",
       "8   3604.959961             0              0         0            1   25   \n",
       "9   1582.949951             0              3         1           10   25   \n",
       "10    57.799999             1              2         0            8   25   \n",
       "11    18.070000             2              3         1            0   25   \n",
       "12  1443.989990             3              4         1            9   25   \n",
       "13  4952.450195             0              0         0            1   25   \n",
       "14    56.400002             1              2         0           21   25   \n",
       "15    11.420000             2              3         1            0   25   \n",
       "16  1414.770020             3              4         1            9   25   \n",
       "17  4271.930176             0              0         0            0   25   \n",
       "18    60.750000             1              2         0           21   25   \n",
       "19    13.480000             2              3         1            0   25   \n",
       "20  1723.959961             0              3         1            8   25   \n",
       "21  2142.770020             3              4         1            3   25   \n",
       "22  4821.609863             0              0         0            0   25   \n",
       "23  1687.869995             0              3         1           14   25   \n",
       "24    66.389999             1              2         0            6   25   \n",
       "\n",
       "          date              k_symbol                   operation    type  \n",
       "0   1993-01-03                   NaN             CASH WITHDRAWAL   DEBIT  \n",
       "1   1993-01-08             HOUSEHOLD  REMITTANCE TO ANOTHER BANK   DEBIT  \n",
       "2   1993-01-11                   NaN              CREDIT IN CASH  CREDIT  \n",
       "3   1993-01-11                   NaN             CASH WITHDRAWAL   DEBIT  \n",
       "4   1993-01-11                   NaN              CREDIT IN CASH  CREDIT  \n",
       "5   1993-01-31     INTEREST CREDITED                         NaN  CREDIT  \n",
       "6   1993-01-31  PAYMENT ON STATEMENT             CASH WITHDRAWAL   DEBIT  \n",
       "7   1993-02-09             HOUSEHOLD  REMITTANCE TO ANOTHER BANK   DEBIT  \n",
       "8   1993-02-10                   NaN              CREDIT IN CASH  CREDIT  \n",
       "9   1993-02-20                   NaN             CASH WITHDRAWAL   DEBIT  \n",
       "10  1993-02-28     INTEREST CREDITED                         NaN  CREDIT  \n",
       "11  1993-02-28  PAYMENT ON STATEMENT             CASH WITHDRAWAL   DEBIT  \n",
       "12  1993-03-09             HOUSEHOLD  REMITTANCE TO ANOTHER BANK   DEBIT  \n",
       "13  1993-03-10                   NaN              CREDIT IN CASH  CREDIT  \n",
       "14  1993-03-31     INTEREST CREDITED                         NaN  CREDIT  \n",
       "15  1993-03-31  PAYMENT ON STATEMENT             CASH WITHDRAWAL   DEBIT  \n",
       "16  1993-04-09             HOUSEHOLD  REMITTANCE TO ANOTHER BANK   DEBIT  \n",
       "17  1993-04-09                   NaN              CREDIT IN CASH  CREDIT  \n",
       "18  1993-04-30     INTEREST CREDITED                         NaN  CREDIT  \n",
       "19  1993-04-30  PAYMENT ON STATEMENT             CASH WITHDRAWAL   DEBIT  \n",
       "20  1993-05-08                   NaN             CASH WITHDRAWAL   DEBIT  \n",
       "21  1993-05-11             HOUSEHOLD  REMITTANCE TO ANOTHER BANK   DEBIT  \n",
       "22  1993-05-11                   NaN              CREDIT IN CASH  CREDIT  \n",
       "23  1993-05-25                   NaN             CASH WITHDRAWAL   DEBIT  \n",
       "24  1993-05-31     INTEREST CREDITED                         NaN  CREDIT  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = seqs_dfs[1]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['categorical_fields', 'START_DATE', 'LOG_AMOUNT_SCALE', 'TD_SCALE', 'ATTR_SCALE', 'K_SYMBOL_TO_NUM', 'NUM_TO_K_SYMBOL', 'n_k_symbols', 'OPERATION_TO_NUM', 'NUM_TO_OPERATION', 'n_operations', 'TYPE_TO_NUM', 'NUM_TO_TYPE', 'n_types', 'field_info'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_encoder.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 40\n",
    "n_seqs_to_generate = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([33., 77., 61., ..., 59., 77., 43.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "real_df = pd.read_csv(f\"stored_data/final_df.csv\", parse_dates=[\"datetime\"])\n",
    "start_date_opts = real_df.groupby(\"account_id\")[\"datetime\"].min().dt.date.to_list()\n",
    "# start_date_opts = [START_DATE + datetime.timedelta(i) for i in range(365)]\n",
    "\n",
    "\n",
    "start_dates = np.random.choice(start_date_opts, size=n_seqs_to_generate)\n",
    "\n",
    "\n",
    "seq_ages = np.random.choice(attributes, size=n_seqs_to_generate)\n",
    "seq_ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin with  v2b__nld_4-dm_128-nh_2-i_0-dr_0.1-opt_adam-lwi_0-bs_64\n",
      "took 1061.0861518383026 secs to generate\n",
      "Wrote df to generated_data/sqrt-log_var__gen_v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64-len_40-v2.csv\n",
      "Begin with  v2b__nld_4-dm_128-nh_2-i_1-dr_0.1-opt_adam-lwi_0-bs_64\n",
      "took 1084.5057411193848 secs to generate\n",
      "Wrote df to generated_data/sqrt-log_var__gen_v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64-len_40-v2.csv\n",
      "Begin with  v2b__nld_4-dm_128-nh_2-i_2-dr_0.1-opt_adam-lwi_0-bs_64\n",
      "took 1144.5846037864685 secs to generate\n",
      "Wrote df to generated_data/sqrt-log_var__gen_v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64-len_40-v2.csv\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(all_models)):\n",
    "    \n",
    "    \n",
    "    \n",
    "    transformer = all_models[i]\n",
    "    \n",
    "    print(\"Begin with \", transformer.id_str)\n",
    "    \n",
    "    save_as = f\"generated_data/sqrt-log_var__gen_{id_str_to_folder(transformer.id_str)}-len_{seq_len}-v2.csv\"\n",
    "    \n",
    "    if os.path.exists(save_as):\n",
    "        print(\"**** Skipping because file already exists. File name =\", save_as, \"\\n\\n\\n\")\n",
    "\n",
    "    start = time.time()\n",
    "    full_df, seqs, raw = generate_seqs(length= seq_len, \n",
    "                                       ages=seq_ages, \n",
    "                                       start_dates= start_dates, \n",
    "                                       return_single_df=True,\n",
    "                                      greedy_dates=True)\n",
    "    \n",
    "    full_df[\"account_id\"] = np.arange(len(full_df)) // seq_len\n",
    "    \n",
    "\n",
    "    print(f\"took {time.time() - start} secs to generate\")\n",
    "\n",
    "\n",
    "    full_df.to_csv(save_as)\n",
    "    print(\"Wrote df to\", save_as)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "transformer.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
